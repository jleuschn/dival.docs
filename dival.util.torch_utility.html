

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>dival.util.torch_utility module &mdash; Deep Inversion Validation Library  documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/custom.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="dival.util.zenodo_download module" href="dival.util.zenodo_download.html" />
    <link rel="prev" title="dival.util.torch_losses module" href="dival.util.torch_losses.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home" alt="Documentation Home"> Deep Inversion Validation Library
          

          
          </a>

          
            
            
              <div class="version">
                0.5.7
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="dival.config.html">dival.config module</a></li>
<li class="toctree-l1"><a class="reference internal" href="dival.data.html">dival.data module</a></li>
<li class="toctree-l1"><a class="reference internal" href="dival.datasets.html">dival.datasets package</a></li>
<li class="toctree-l1"><a class="reference internal" href="dival.datasets.angle_subset_dataset.html">dival.datasets.angle_subset_dataset module</a></li>
<li class="toctree-l1"><a class="reference internal" href="dival.datasets.cached_dataset.html">dival.datasets.cached_dataset module</a></li>
<li class="toctree-l1"><a class="reference internal" href="dival.datasets.dataset.html">dival.datasets.dataset module</a></li>
<li class="toctree-l1"><a class="reference internal" href="dival.datasets.ellipses_dataset.html">dival.datasets.ellipses_dataset module</a></li>
<li class="toctree-l1"><a class="reference internal" href="dival.datasets.fbp_dataset.html">dival.datasets.fbp_dataset module</a></li>
<li class="toctree-l1"><a class="reference internal" href="dival.datasets.lodopab_dataset.html">dival.datasets.lodopab_dataset module</a></li>
<li class="toctree-l1"><a class="reference internal" href="dival.datasets.reordered_dataset.html">dival.datasets.reordered_dataset module</a></li>
<li class="toctree-l1"><a class="reference internal" href="dival.datasets.standard.html">dival.datasets.standard module</a></li>
<li class="toctree-l1"><a class="reference internal" href="dival.evaluation.html">dival.evaluation module</a></li>
<li class="toctree-l1"><a class="reference internal" href="dival.hyper_param_search.html">dival.hyper_param_search module</a></li>
<li class="toctree-l1"><a class="reference internal" href="dival.measure.html">dival.measure module</a></li>
<li class="toctree-l1"><a class="reference internal" href="dival.reconstructors.html">dival.reconstructors package</a></li>
<li class="toctree-l1"><a class="reference internal" href="dival.reconstructors.dip_ct_reconstructor.html">dival.reconstructors.dip_ct_reconstructor module</a></li>
<li class="toctree-l1"><a class="reference internal" href="dival.reconstructors.fbpunet_reconstructor.html">dival.reconstructors.fbpunet_reconstructor module</a></li>
<li class="toctree-l1"><a class="reference internal" href="dival.reconstructors.iradonmap_reconstructor.html">dival.reconstructors.iradonmap_reconstructor module</a></li>
<li class="toctree-l1"><a class="reference internal" href="dival.reconstructors.learnedgd_reconstructor.html">dival.reconstructors.learnedgd_reconstructor module</a></li>
<li class="toctree-l1"><a class="reference internal" href="dival.reconstructors.learnedpd_reconstructor.html">dival.reconstructors.learnedpd_reconstructor module</a></li>
<li class="toctree-l1"><a class="reference internal" href="dival.reconstructors.networks.html">dival.reconstructors.networks package</a></li>
<li class="toctree-l1"><a class="reference internal" href="dival.reconstructors.networks.iradonmap.html">dival.reconstructors.networks.iradonmap module</a></li>
<li class="toctree-l1"><a class="reference internal" href="dival.reconstructors.networks.iterative.html">dival.reconstructors.networks.iterative module</a></li>
<li class="toctree-l1"><a class="reference internal" href="dival.reconstructors.networks.unet.html">dival.reconstructors.networks.unet module</a></li>
<li class="toctree-l1"><a class="reference internal" href="dival.reconstructors.odl_reconstructors.html">dival.reconstructors.odl_reconstructors module</a></li>
<li class="toctree-l1"><a class="reference internal" href="dival.reconstructors.reconstructor.html">dival.reconstructors.reconstructor module</a></li>
<li class="toctree-l1"><a class="reference internal" href="dival.reconstructors.regression_reconstructors.html">dival.reconstructors.regression_reconstructors module</a></li>
<li class="toctree-l1"><a class="reference internal" href="dival.reconstructors.standard_learned_reconstructor.html">dival.reconstructors.standard_learned_reconstructor module</a></li>
<li class="toctree-l1"><a class="reference internal" href="dival.reconstructors.tvadam_ct_reconstructor.html">dival.reconstructors.tvadam_ct_reconstructor module</a></li>
<li class="toctree-l1"><a class="reference internal" href="dival.reference_reconstructors.html">dival.reference_reconstructors module</a></li>
<li class="toctree-l1"><a class="reference internal" href="dival.util.html">dival.util package</a></li>
<li class="toctree-l1"><a class="reference internal" href="dival.util.constants.html">dival.util.constants module</a></li>
<li class="toctree-l1"><a class="reference internal" href="dival.util.download.html">dival.util.download module</a></li>
<li class="toctree-l1"><a class="reference internal" href="dival.util.input.html">dival.util.input module</a></li>
<li class="toctree-l1"><a class="reference internal" href="dival.util.odl_noise_random_state.html">dival.util.odl_noise_random_state module</a></li>
<li class="toctree-l1"><a class="reference internal" href="dival.util.odl_utility.html">dival.util.odl_utility module</a></li>
<li class="toctree-l1"><a class="reference internal" href="dival.util.plot.html">dival.util.plot module</a></li>
<li class="toctree-l1"><a class="reference internal" href="dival.util.std_out_err_redirect_tqdm.html">dival.util.std_out_err_redirect_tqdm module</a></li>
<li class="toctree-l1"><a class="reference internal" href="dival.util.torch_losses.html">dival.util.torch_losses module</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">dival.util.torch_utility module</a></li>
<li class="toctree-l1"><a class="reference internal" href="dival.util.zenodo_download.html">dival.util.zenodo_download module</a></li>
<li class="toctree-l1"><a class="reference internal" href="dival.version.html">dival.version module</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Deep Inversion Validation Library</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="dival.util.html">dival.util package</a> &raquo;</li>
        
      <li>dival.util.torch_utility module</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/dival.util.torch_utility.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="module-dival.util.torch_utility">
<span id="dival-util-torch-utility-module"></span><h1>dival.util.torch_utility module<a class="headerlink" href="#module-dival.util.torch_utility" title="Permalink to this headline">¶</a></h1>
<p>Provides utilities related to PyTorch.</p>
<p>The classes and functions</p>
<blockquote>
<div><p><a class="reference internal" href="#dival.util.torch_utility.TorchRayTrafoParallel2DModule" title="dival.util.torch_utility.TorchRayTrafoParallel2DModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">TorchRayTrafoParallel2DModule</span></code></a>
<a class="reference internal" href="#dival.util.torch_utility.TorchRayTrafoParallel2DAdjointModule" title="dival.util.torch_utility.TorchRayTrafoParallel2DAdjointModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">TorchRayTrafoParallel2DAdjointModule</span></code></a>
<a class="reference internal" href="#dival.util.torch_utility.get_torch_ray_trafo_parallel_2d" title="dival.util.torch_utility.get_torch_ray_trafo_parallel_2d"><code class="xref py py-func docutils literal notranslate"><span class="pre">get_torch_ray_trafo_parallel_2d()</span></code></a>
<a class="reference internal" href="#dival.util.torch_utility.get_torch_ray_trafo_parallel_2d_adjoint" title="dival.util.torch_utility.get_torch_ray_trafo_parallel_2d_adjoint"><code class="xref py py-func docutils literal notranslate"><span class="pre">get_torch_ray_trafo_parallel_2d_adjoint()</span></code></a>.</p>
</div></blockquote>
<p>in this module rely on the
<a class="reference external" href="https://github.com/ahendriksen/tomosipo">tomosipo</a> library and experimental
astra features available in version 1.9.9.dev4 using CUDA.
In order to instantiate or call these classes and functions, all of these
requirements need to be fulfilled, otherwise an <code class="xref py py-class docutils literal notranslate"><span class="pre">ImportError</span></code> is raised.</p>
<dl class="py class">
<dt id="dival.util.torch_utility.TorchRayTrafoParallel2DModule">
<em class="property">class </em><code class="sig-prename descclassname">dival.util.torch_utility.</code><code class="sig-name descname">TorchRayTrafoParallel2DModule</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ray_trafo</span></em>, <em class="sig-param"><span class="n">init_z_shape</span><span class="o">=</span><span class="default_value">1</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/dival/util/torch_utility.html#TorchRayTrafoParallel2DModule"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#dival.util.torch_utility.TorchRayTrafoParallel2DModule" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Torch module applying a 2D parallel-beam ray transform using tomosipo that
calls the direct forward projection routine of astra, which avoids copying
between GPU and CPU (available in 1.9.9.dev4).</p>
<p>All 2D transforms are computed using a single 3D transform.
To this end the used tomosipo operator is renewed in <a class="reference internal" href="#dival.util.torch_utility.TorchRayTrafoParallel2DModule.forward" title="dival.util.torch_utility.TorchRayTrafoParallel2DModule.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code></a>
everytime the product of batch and channel dimensions of the current batch
differs compared to the previous batch, or compared to the value of
<cite>init_z_shape</cite> specified to <code class="xref py py-meth docutils literal notranslate"><span class="pre">init()</span></code> for the first batch.</p>
<dl class="py method">
<dt id="dival.util.torch_utility.TorchRayTrafoParallel2DModule.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ray_trafo</span></em>, <em class="sig-param"><span class="n">init_z_shape</span><span class="o">=</span><span class="default_value">1</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/dival/util/torch_utility.html#TorchRayTrafoParallel2DModule.__init__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#dival.util.torch_utility.TorchRayTrafoParallel2DModule.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ray_trafo</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">odl.tomo.RayTransform</span></code>) – Ray transform</p></li>
<li><p><strong>init_z_shape</strong> (<em>int</em><em>, </em><em>optional</em>) – Initial guess for the number of 2D transforms per batch, i.e. the
product of batch and channel dimensions.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="dival.util.torch_utility.TorchRayTrafoParallel2DModule.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/dival/util/torch_utility.html#TorchRayTrafoParallel2DModule.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#dival.util.torch_utility.TorchRayTrafoParallel2DModule.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt id="dival.util.torch_utility.TorchRayTrafoParallel2DModule.training">
<code class="sig-name descname">training</code><em class="property">: bool</em><a class="headerlink" href="#dival.util.torch_utility.TorchRayTrafoParallel2DModule.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt id="dival.util.torch_utility.TorchRayTrafoParallel2DAdjointModule">
<em class="property">class </em><code class="sig-prename descclassname">dival.util.torch_utility.</code><code class="sig-name descname">TorchRayTrafoParallel2DAdjointModule</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ray_trafo</span></em>, <em class="sig-param"><span class="n">init_z_shape</span><span class="o">=</span><span class="default_value">1</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/dival/util/torch_utility.html#TorchRayTrafoParallel2DAdjointModule"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#dival.util.torch_utility.TorchRayTrafoParallel2DAdjointModule" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Torch module applying the adjoint of a 2D parallel-beam ray transform
using tomosipo that calls the direct backward projection routine of astra,
which avoids copying between GPU and CPU (available in 1.9.9.dev4).</p>
<p>All 2D transforms are computed using a single 3D transform.
To this end the used tomosipo operator is renewed in <a class="reference internal" href="#dival.util.torch_utility.TorchRayTrafoParallel2DAdjointModule.forward" title="dival.util.torch_utility.TorchRayTrafoParallel2DAdjointModule.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code></a>
everytime the product of batch and channel dimensions of the current batch
differs compared to the previous batch, or compared to the value of
<cite>init_z_shape</cite> specified to <code class="xref py py-meth docutils literal notranslate"><span class="pre">init()</span></code> for the first batch.</p>
<dl class="py method">
<dt id="dival.util.torch_utility.TorchRayTrafoParallel2DAdjointModule.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ray_trafo</span></em>, <em class="sig-param"><span class="n">init_z_shape</span><span class="o">=</span><span class="default_value">1</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/dival/util/torch_utility.html#TorchRayTrafoParallel2DAdjointModule.__init__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#dival.util.torch_utility.TorchRayTrafoParallel2DAdjointModule.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ray_trafo</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">odl.tomo.RayTransform</span></code>) – Ray transform</p></li>
<li><p><strong>init_z_shape</strong> (<em>int</em><em>, </em><em>optional</em>) – Initial guess for the number of 2D transforms per batch, i.e. the
product of batch and channel dimensions.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="dival.util.torch_utility.TorchRayTrafoParallel2DAdjointModule.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/dival/util/torch_utility.html#TorchRayTrafoParallel2DAdjointModule.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#dival.util.torch_utility.TorchRayTrafoParallel2DAdjointModule.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt id="dival.util.torch_utility.TorchRayTrafoParallel2DAdjointModule.training">
<code class="sig-name descname">training</code><em class="property">: bool</em><a class="headerlink" href="#dival.util.torch_utility.TorchRayTrafoParallel2DAdjointModule.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py function">
<dt id="dival.util.torch_utility.get_torch_ray_trafo_parallel_2d">
<code class="sig-prename descclassname">dival.util.torch_utility.</code><code class="sig-name descname">get_torch_ray_trafo_parallel_2d</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ray_trafo</span></em>, <em class="sig-param"><span class="n">z_shape</span><span class="o">=</span><span class="default_value">1</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/dival/util/torch_utility.html#get_torch_ray_trafo_parallel_2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#dival.util.torch_utility.get_torch_ray_trafo_parallel_2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a torch autograd-enabled function from a 2D parallel-beam
<code class="xref py py-class docutils literal notranslate"><span class="pre">odl.tomo.RayTransform</span></code> using tomosipo that calls the direct
forward projection routine of astra, which avoids copying between GPU and
CPU (available in 1.9.9.dev4).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ray_trafo</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">odl.tomo.RayTransform</span></code>) – Ray transform</p></li>
<li><p><strong>z_shape</strong> (<em>int</em><em>, </em><em>optional</em>) – Channel dimension.
Default: <code class="docutils literal notranslate"><span class="pre">1</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>torch_ray_trafo</strong> – Torch autograd-enabled function applying the parallel-beam forward
projection.
Input and output have a trivial leading batch dimension and a channel
dimension specified by <cite>z_shape</cite> (default <code class="docutils literal notranslate"><span class="pre">1</span></code>), i.e. the
input shape is <code class="docutils literal notranslate"><span class="pre">(1,</span> <span class="pre">z_shape)</span> <span class="pre">+</span> <span class="pre">ray_trafo.domain.shape</span></code> and the
output shape is <code class="docutils literal notranslate"><span class="pre">(1,</span> <span class="pre">z_shape)</span> <span class="pre">+</span> <span class="pre">ray_trafo.range.shape</span></code>.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>callable</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="dival.util.torch_utility.get_torch_ray_trafo_parallel_2d_adjoint">
<code class="sig-prename descclassname">dival.util.torch_utility.</code><code class="sig-name descname">get_torch_ray_trafo_parallel_2d_adjoint</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ray_trafo</span></em>, <em class="sig-param"><span class="n">z_shape</span><span class="o">=</span><span class="default_value">1</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/dival/util/torch_utility.html#get_torch_ray_trafo_parallel_2d_adjoint"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#dival.util.torch_utility.get_torch_ray_trafo_parallel_2d_adjoint" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a torch autograd-enabled function from a 2D parallel-beam
<code class="xref py py-class docutils literal notranslate"><span class="pre">odl.tomo.RayTransform</span></code> using tomosipo that calls the direct
backward projection routine of astra, which avoids copying between GPU and
CPU (available in 1.9.9.dev4).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ray_trafo</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">odl.tomo.RayTransform</span></code>) – Ray transform</p></li>
<li><p><strong>z_shape</strong> (<em>int</em><em>, </em><em>optional</em>) – Batch dimension.
Default: <code class="docutils literal notranslate"><span class="pre">1</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>torch_ray_trafo_adjoint</strong> – Torch autograd-enabled function applying the parallel-beam backward
projection.
Input and output have a trivial leading batch dimension and a channel
dimension specified by <cite>z_shape</cite> (default <code class="docutils literal notranslate"><span class="pre">1</span></code>), i.e. the
input shape is <code class="docutils literal notranslate"><span class="pre">(1,</span> <span class="pre">z_shape)</span> <span class="pre">+</span> <span class="pre">ray_trafo.range.shape</span></code> and the
output shape is <code class="docutils literal notranslate"><span class="pre">(1,</span> <span class="pre">z_shape)</span> <span class="pre">+</span> <span class="pre">ray_trafo.domain.shape</span></code>.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>callable</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="dival.util.torch_utility.load_state_dict_convert_data_parallel">
<code class="sig-prename descclassname">dival.util.torch_utility.</code><code class="sig-name descname">load_state_dict_convert_data_parallel</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">model</span></em>, <em class="sig-param"><span class="n">state_dict</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/dival/util/torch_utility.html#load_state_dict_convert_data_parallel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#dival.util.torch_utility.load_state_dict_convert_data_parallel" title="Permalink to this definition">¶</a></dt>
<dd><p>Load a state dict into a model, while automatically converting the weight
names if <code class="xref py py-attr docutils literal notranslate"><span class="pre">model</span></code> is a <code class="xref py py-class docutils literal notranslate"><span class="pre">nn.DataParallel</span></code>-model but the stored
state dict stems from a non-data-parallel model, or vice versa.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<em>nn.Module</em>) – Torch model that should load the state dict.</p></li>
<li><p><strong>state_dict</strong> (<em>dict</em>) – Torch state dict</p></li>
</ul>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>RuntimeError</strong> – If there are missing or unexpected keys in the state dict.
    This error is not raised when conversion of the weight names succeeds.</p>
</dd>
</dl>
</dd></dl>

</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="dival.util.zenodo_download.html" class="btn btn-neutral float-right" title="dival.util.zenodo_download module" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="dival.util.torch_losses.html" class="btn btn-neutral float-left" title="dival.util.torch_losses module" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2020, Johannes Leuschner, Maximilian Schmidt, Daniel Otero Baguer, David Erzmann

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>